# Replicable News Aggregator Project Guide

## 1. Introduction

This guide details the architecture and development process of an automated news aggregation website. It is designed to be used by developers or an AI assistant to replicate this project for any chosen niche. The goal is to create a static HTML website that automatically fetches, categorizes, and displays news from various sources daily.

**Core Functionality:**
*   Fetches news from RSS feeds, NewsAPIs, and Reddit.
*   Filters and categorizes content based on keywords.
*   Generates static HTML pages (main index, daily archives).
*   Automates daily updates via GitHub Actions.
*   Includes SEO best practices (sitemap, meta tags).

## 2. Project Architecture Overview

The project is built with Python and leverages several key libraries and tools:

*   **Backend Logic (`aggregator.py`):**
    *   **Python:** Core programming language.
    *   **`requests`:** For making HTTP requests to APIs and RSS feeds.
    *   **`feedparser`:** For parsing RSS/Atom feeds.
    *   **`praw`:** For interacting with the Reddit API.
    *   **`jinja2`:** For templating HTML pages.
    *   **`langdetect`:** For filtering non-English content.
    *   Standard libraries: `os`, `datetime`, `json`, `re`.
*   **Configuration (`config.json`):**
    *   Manages excluded sites, low-quality URL patterns, and negative keywords.
*   **Frontend (`template.html`, `archive_index_template.html`, CSS):**
    *   Static HTML pages generated by the backend.
    *   Styling is done via CSS (can be embedded or separate).
*   **Automation (`.github/workflows/daily-update.yml`):**
    *   GitHub Actions workflow for daily execution of the aggregation script.
*   **SEO (`generate_sitemap.py`, `sitemap_template.xml`, `robots.txt`):**
    *   Scripts and templates for generating sitemaps and guiding search engine crawlers.
*   **Dependencies (`requirements.txt`):**
    *   Lists all necessary Python packages.

## 3. Replication Process: Building for a New Niche

To adapt this project for a new niche, an AI or developer will need to gather specific information about the target niche and then configure the project components accordingly.

### Phase 1: Niche Definition & Initial Configuration

This is the most critical phase. The AI/developer needs to define the scope of the new site.

**Questions to Ask the User (or for the AI to determine):**

1.  **Primary Niche/Topic:** What is the central theme of the news site (e.g., "Triathlon & Endurance Sports", "Sustainable Technology", "Indie Game Development")?
2.  **Site Name/Brand:** What will the website be called?
3.  **Key Categories:** What are the main sub-topics or categories within this niche (e.g., for Triathlon: "Race Results", "Training Tips", "Gear Reviews", "Nutrition")?
4.  **Target Keywords:**
    *   What are 3-5 primary keywords for this niche?
    *   What are 5-10 secondary keywords?
    *   What are some relevant long-tail keywords?
5.  **Specific RSS Feeds:** Can you list at least 5-10 trusted RSS feeds for this niche (blogs, official organizations, news sites)?
6.  **Relevant Reddit Subreddits:** Can you list 3-5 relevant Reddit subreddits?
7.  **General News Query (for NewsAPI):** Formulate a broad query string for NewsAPI to capture general news related to the niche (e.g., `'(triathlon OR ironman) AND (news OR race OR training)'`).
8.  **Negative Keywords:** Are there any specific keywords that should *exclude* an article (e.g., for a "Sustainable Technology" site, perhaps "cryptocurrency mining" if not desired)?
9.  **Excluded Sites/Domains:** Are there any specific websites that should be ignored?

**Configuration Steps:**

Once the above information is gathered:

1.  **Update `aggregator.py` Constants:**
    *   `NEWS_API_QUERY`: Set based on question #7.
    *   `SUBREDDITS`: Set based on question #6.
    *   `MAX_REDDIT_POSTS_PER_SUB`: Adjust as needed (default is low, can be increased).
    *   `RSS_FEEDS`: Populate the dictionary with niche-specific feeds from question #5.
        ```python
        RSS_FEEDS = {
            "Niche Source 1 Name": "URL_to_RSS_feed_1",
            "Niche Source 2 Name": "URL_to_RSS_feed_2",
            # ... add all feeds
        }
        ```
    *   `CATEGORIES`: Define a list of category names based on question #3.
        ```python
        CATEGORIES = [
            'Niche Category 1',
            'Niche Category 2',
            # ... add all categories
        ]
        ```
    *   `CATEGORY_KEYWORDS`: This is crucial. Create a dictionary where keys are the `CATEGORIES` defined above, and values are lists of keywords that assign an article to that category. This often requires iteration and refinement.
        ```python
        CATEGORY_KEYWORDS = {
            'Niche Category 1': ['keyword1', 'keyword2', 'phrase for category 1'],
            'Niche Category 2': ['keyword3', 'keyword4'],
            # ... for all categories
        }
        ```
    *   `TRENDING_KEYWORDS`: (Optional) Keywords that might push an article into a "Trending" category.
    *   `MAX_HEADLINE_WORDS`: Adjust if needed (default: 8).

2.  **Update `config.json`:**
    *   `excluded_sites`: Add any domains to ignore based on question #9.
    *   `negative_keywords`: Add keywords that should filter out articles, based on question #8.
    *   `low_quality_url_patterns`: Keep the defaults unless specific patterns for the niche are known.

3.  **Update `template.html` (and `archive_index_template.html`):**
    *   **Site Title:** Change `<title>AI Flash Report - Daily AI News</title>` to reflect the new niche and site name.
    *   **Meta Description & Keywords:**
        *   `<meta name="description" content="...">`: Update with a compelling description using primary and secondary keywords for the new niche.
        *   `<meta name="keywords" content="...">`: Update with the target keywords from question #4.
    *   **Open Graph & Twitter Card Tags:**
        *   Update `og:title`, `og:description`, `og:image`, `og:url`, `og:site_name`.
        *   Update `twitter:title`, `twitter:description`, `twitter:image`.
        *   *Ensure placeholder image paths (`/images/og-image.png`, etc.) are noted so the user knows to create these images.*
    *   **Schema.org Structured Data:**
        *   Update `headline`, `publisher.name`, `publisher.logo.url`, and `description` to match the new niche.
        *   *Note the need for a `logo.png` in `/images/`.*
    *   **Site Header/Logo:** Update `<h1>` and any logo elements.
    *   **Footer:** Update copyright and any links.
    *   **Category Descriptions (if used in template):** If the template includes static text per category, ensure this is updated or made dynamic.

4.  **Update `README.md`:**
    *   Reflect the new project's name, niche, and purpose.
    *   Update setup instructions if any specific changes were made.

5.  **Update `generate_sitemap.py` & `sitemap_template.xml`:**
    *   Ensure the base URL in `sitemap_template.xml` (`<loc>https://aiflashreport.com...</loc>`) is changed to the new site's domain. The `generate_sitemap.py` script will use this template.

6.  **Update `robots.txt`:**
    *   Ensure `Sitemap:` directive points to the correct `sitemap.xml` URL for the new domain.

### Phase 2: Technical Setup

1.  **Clone/Copy Base Project:** Start with a copy of the original project files.
2.  **Python Virtual Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    The `requirements.txt` should contain:
    ```
    requests
    feedparser
    praw
    Jinja2
    langdetect
    # Add other specific versions if necessary
    ```
4.  **API Keys & Environment Variables:**
    The script requires API keys for NewsAPI and Reddit. These must be set as environment variables.
    *   `NEWS_API_KEY`
    *   `REDDIT_CLIENT_ID`
    *   `REDDIT_CLIENT_SECRET`
    *   `REDDIT_USER_AGENT` (e.g., `"web:yournicheaggregator:v0.1 (by /u/yourusername)"`)
    *   `PERPLEXITY_API_KEY` (Optional, can be removed if not used for the new niche)

    These can be set in the shell or a `.env` file (ensure `.env` is in `.gitignore`).

### Phase 3: Customization & Branding

1.  **CSS Styling:** Modify the CSS in `template.html` (or a linked `style.css` file if separated) to match the desired look and feel for the new niche.
2.  **Images:** Create and place the necessary images:
    *   `/images/og-image.png` (1200x630px)
    *   `/images/twitter-card.png` (1200x600px)
    *   `/images/logo.png` (for Schema.org and site header)
    *   Favicon files (e.g., `favicon.ico`, `favicon-16x16.png`, etc.)
3.  **Content for Static Pages:**
    *   Update `about.html` with relevant information for the new site.
    *   Create any other static pages needed.

### Phase 4: Automation & Deployment

1.  **GitHub Repository:** Create a new GitHub repository for the project.
2.  **GitHub Actions Workflow (`.github/workflows/daily-update.yml`):**
    *   This file can largely be reused.
    *   **Crucial:** Set up GitHub Secrets in the new repository settings for all the API keys used in the `env` section of the workflow (`NEWS_API_KEY`, `REDDIT_CLIENT_ID`, etc.).
3.  **Hosting:**
    *   **Static Hosting Providers:** Netlify, GitHub Pages, Vercel, Cloudflare Pages are good options.
    *   Configure the chosen provider to build/deploy from the `main` branch of the GitHub repository. Usually, no complex build command is needed as HTML is committed directly.

### Phase 5: Testing, Maintenance & Iteration

1.  **Local Testing:** Run `python3 aggregator.py` locally to test data fetching, processing, and HTML generation. View `index.html` in a browser.
2.  **Deployment Testing:** Push changes to GitHub and monitor the GitHub Actions workflow. Check the live site after deployment.
3.  **Ongoing Maintenance:**
    *   Monitor for broken RSS feeds or API issues.
    *   Refine `CATEGORY_KEYWORDS` and `negative_keywords` based on the quality of aggregated content.
    *   Update dependencies and Python version as needed.
    *   Check Google Search Console for SEO issues and performance.

## 4. Developer & Project Rules (Best Practices)

These are inherited from the original project and are crucial for smooth development, especially with automated content generation.

*   **Git Workflow:**
    *   **Main Branch:** The `main` branch is the source of truth and what gets deployed.
    *   **Feature Branches:** Make changes on separate branches and merge via Pull Requests (optional for solo developers but good practice).
    *   **API Keys:** NEVER commit API keys or sensitive credentials. Use environment variables locally and GitHub Secrets for Actions.
    *   **Generated Files:**
        *   `index.html`, `archive/*.html`, `sitemap.xml` are auto-generated.
        *   **DO NOT MANUALLY EDIT THESE FILES.**
        *   Before committing changes to *code* (`aggregator.py`) or *templates* (`template.html`), if you've run the script locally, **you MUST restore/discard changes to these generated files**:
            ```bash
            git restore index.html archive/ sitemap.xml
            # or git checkout -- index.html archive/ sitemap.xml
            ```
        *   The GitHub Action is responsible for generating the final versions of these files.
    *   **Pull Before Push:** Always `git pull --rebase` (or `git pull`) before pushing changes to avoid conflicts, especially since the remote can be updated by the GitHub Action.
*   **Code Standards:**
    *   Keep `aggregator.py` well-commented, especially the data fetching and categorization logic.
    *   Ensure robust error handling (e.g., what happens if an RSS feed is down or an API key is invalid?). The script should ideally continue with other sources.
    *   Make configuration (keywords, sources) easy to update, primarily through `aggregator.py` constants and `config.json`.
*   **SEO Maintenance:**
    *   Regularly review meta descriptions and keywords in `template.html`.
    *   Submit `sitemap.xml` to Google Search Console.
    *   Monitor indexing status and crawl errors.

## 5. Example: Adapting for "Sustainable Technology News"

**AI/User Answers:**
1.  **Primary Niche:** Sustainable Technology
2.  **Site Name:** GreenTech Today
3.  **Key Categories:** Renewable Energy, Green Transportation, Sustainable Building, Circular Economy, Climate Tech
4.  **Keywords:**
    *   Primary: sustainable technology, green tech news, climate tech
    *   Secondary: renewable energy, electric vehicles, sustainable innovation
    *   Long-tail: latest advancements in solar power, circular economy business models
5.  **RSS Feeds:** `{"CleanTechnica": "...", "GreenBiz": "...", ...}`
6.  **Subreddits:** `["sustainability", "ClimateAction", "RenewableEnergy"]`
7.  **NewsAPI Query:** `'(sustainable OR sustainability OR "green tech" OR "climate tech" OR renewable) AND (technology OR innovation OR news OR breakthrough)'`
8.  **Negative Keywords:** `["politics", "fossil fuels investment"]`
9.  **Excluded Sites:** `["old-biased-site.com"]`

**AI/Developer Actions:**
*   Updates `RSS_FEEDS`, `CATEGORIES`, `CATEGORY_KEYWORDS`, `NEWS_API_QUERY` in `aggregator.py`.
*   Updates `negative_keywords`, `excluded_sites` in `config.json`.
*   Changes titles, meta tags, and branding in `template.html` to "GreenTech Today".
*   Creates `logo.png`, `og-image.png`, `twitter-card.png` with GreenTech Today branding.
*   Sets up new GitHub repo, secrets, and deploys via Netlify/GitHub Pages.

This guide provides a comprehensive framework. The key to success is careful configuration in Phase 1 and iterative refinement of keywords and sources based on the aggregated content quality.

## 6. Additional Notes & Lessons Learned
Okay, this has been a comprehensive project, and we've navigated several interesting challenges to get your triathlon news aggregator up and running! Reflecting on the process, here are some key takeaways and considerations if you were to build a similar news feed for a different site or niche:
I. Core Architecture & Content Generation:
Static First for Simplicity & Performance:
Lesson: Generating static HTML files (like index.html for the main news, daily archives YYYY-MM-DD.html, and archive.html) is a robust approach for a news feed. It's fast, SEO-friendly, and simplifies hosting.
Tools: Python proved effective for scripting the aggregation logic, and Jinja2 for templating the HTML. This combination is lightweight and powerful.
Consideration: For a different niche, assess if this static approach still fits. If you need real-time updates or heavy user interaction on the news pages themselves, a static generator might still be part of a larger system, or you might explore dynamic rendering.
Content Sources (RSS vs. APIs):
Lesson: We started with plans for NewsAPI/Reddit but pivoted to RSS feeds for initial simplicity. RSS is often easier to integrate (no API keys initially) and widely available.
Consideration: For a new niche:
Research available RSS feeds. Their quality and update frequency vary.
Evaluate APIs: They offer more structured data and potentially richer content but come with API key management, rate limits, and terms of service. Plan for secure secret storage (like GitHub Secrets) from the start if using APIs.
Content Processing & Filtering:
Lesson: Your aggregator.py script handles fetching, filtering (language, duplicates via `processed_urls.json`, excluded content), and categorization. This is crucial for quality control.
Consideration: For a new niche, the filtering and categorization logic (keywords in config.json, `processed_urls.json` for deduplication) will need to be tailored. The definition of "relevant" news will change.
II. Automation & Deployment (GitHub Actions & Netlify):
Automated Content Updates:
Lesson: GitHub Actions is excellent for automating the daily fetching and generation of news. The workflow (daily-update.yml) handles checkout, Python setup, dependency installation, script execution, and committing generated files back to the repository.
Key Pitfall: Pathing within the GitHub Action runner can be tricky. Remember that commands in run steps usually execute from the repository root. Paths like public/news/ are relative to this root, not ../public/news/.
Permissions: Ensure the Action has permissions: contents: write to push changes.
Integrating Static Content with an SPA (Vite/React on Netlify):
Lesson: The public directory in your Vite project is the key. Files placed here (by the GitHub Action, in our case) are copied to the root of your build output (dist) and served by Netlify.
CRUCIAL - Netlify Redirects (netlify.toml): For SPAs, Netlify's default behavior is often to redirect all paths to your app's index.html for client-side routing. This causes 404s for static files/directories if not handled.
You must add specific redirect rules for your static content paths (e.g., /news/, /news/archive.html, /sitemap.xml) before the general SPA catch-all rule (/* /index.html 200). This tells Netlify to serve these static files directly.
Consideration: This SPA integration pattern is common. Always be mindful of how your hosting provider handles static files versus SPA routing.
Sitemap Generation:
Lesson: We initially had a broken sitemap.xml. A React component (Sitemap.tsx) is great for an HTML sitemap for users, but search engines need a valid sitemap.xml.
Solution: We created a separate Python script (generate_sitemap.py) to build the sitemap.xml in the correct format and integrated its execution into the GitHub Action.
Key Pitfall: XML syntax is strict. Ensure your generation script produces well-formed XML. We hit a Python f-string syntax error (f-string expression part cannot include a backslash) which was a subtle but critical bug in the generation.
Automation: The GitHub Action now also updates sitemap.xml, ensuring new daily archives will eventually be included (the script has a TODO for dynamically adding these).
III. Development Workflow & Best Practices:
Git Workflow with Automated Commits:
Lesson: When a GitHub Action commits to your repository, your local copy can quickly become out of sync. This was a recurring theme.
Best Practice (as documented in `git-workflow.mdc`):
Always git pull (ideally git pull --rebase for a cleaner history) before starting work and especially before pushing.
CRITICAL: Do not manually edit or commit files that are auto-generated by your scripts (e.g., index.html, daily archives, sitemap.xml in the public directory). If you run the generator scripts locally for testing, git restore <generated_file_paths> these files before committing your code changes (e.g., changes to aggregator.py or HTML templates). This prevents conflicts with the versions generated and committed by the GitHub Action.
The `git-workflow.mdc` you created is a valuable asset for this kind of project.
Configuration Management:
Lesson: Using config.json for RSS feeds and keywords allows for easier updates without code changes.
Consideration: For a new niche, this configuration-driven approach is highly recommended.
Local Preview & Testing:
Lesson: Using python -m http.server from the triathlon-news/output directory was helpful for a quick local preview of the generated static news pages.
Consideration: Be aware that local server paths might not perfectly mirror the live site structure (e.g., accessing archive.html locally vs. /news/archive.html live). This is normal but good to keep in mind when debugging links.
Error Debugging:
Lesson: We used GitHub Action logs, Netlify deploy logs, and browser developer tools extensively. These are indispensable.
Common Issues Encountered: Jinja2 undefined variables, Git merge conflicts, path issues in scripts/workflows, SPA routing conflicts, and XML syntax errors.
If building another news feed, I would particularly emphasize:
Planning the Netlify redirects early if integrating with an SPA.
Establishing a strict Git workflow from day one to handle automated commits from the Action, clearly documenting which files are "source code" vs. "generated artifacts."
Thoroughly testing any content generation scripts (like sitemap.xml generator) for correct output format before integrating them into the automated workflow.
