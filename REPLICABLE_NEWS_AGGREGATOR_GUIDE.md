# Replicable News Aggregator Project Guide

## 1. Introduction

This guide details the architecture and development process of an automated news aggregation website. It is designed to be used by developers or an AI assistant to replicate this project for any chosen niche. The goal is to create a static HTML website that automatically fetches, categorizes, and displays news from various sources daily.

**Core Functionality:**
*   Fetches news from RSS feeds, NewsAPIs, and Reddit.
*   Filters and categorizes content based on keywords.
*   Generates static HTML pages (main index, daily archives).
*   Automates daily updates via GitHub Actions.
*   Includes SEO best practices (sitemap, meta tags).

## 2. Project Architecture Overview

The project is built with Python and leverages several key libraries and tools:

*   **Backend Logic (`aggregator.py`):**
    *   **Python:** Core programming language.
    *   **`requests`:** For making HTTP requests to APIs and RSS feeds.
    *   **`feedparser`:** For parsing RSS/Atom feeds.
    *   **`praw`:** For interacting with the Reddit API.
    *   **`jinja2`:** For templating HTML pages.
    *   **`langdetect`:** For filtering non-English content.
    *   Standard libraries: `os`, `datetime`, `json`, `re`.
*   **Configuration (`config.json`):**
    *   Manages excluded sites, low-quality URL patterns, and negative keywords.
*   **Frontend (`template.html`, `archive_index_template.html`, CSS):**
    *   Static HTML pages generated by the backend.
    *   Styling is done via CSS (can be embedded or separate).
*   **Automation (`.github/workflows/daily-update.yml`):**
    *   GitHub Actions workflow for daily execution of the aggregation script.
*   **SEO (`generate_sitemap.py`, `sitemap_template.xml`, `robots.txt`):**
    *   Scripts and templates for generating sitemaps and guiding search engine crawlers.
*   **Dependencies (`requirements.txt`):**
    *   Lists all necessary Python packages.

## 3. Replication Process: Building for a New Niche

To adapt this project for a new niche, an AI or developer will need to gather specific information about the target niche and then configure the project components accordingly.

### Phase 1: Niche Definition & Initial Configuration

This is the most critical phase. The AI/developer needs to define the scope of the new site.

**Questions to Ask the User (or for the AI to determine):**

1.  **Primary Niche/Topic:** What is the central theme of the news site (e.g., "Triathlon & Endurance Sports", "Sustainable Technology", "Indie Game Development")?
2.  **Site Name/Brand:** What will the website be called?
3.  **Key Categories:** What are the main sub-topics or categories within this niche (e.g., for Triathlon: "Race Results", "Training Tips", "Gear Reviews", "Nutrition")?
4.  **Target Keywords:**
    *   What are 3-5 primary keywords for this niche?
    *   What are 5-10 secondary keywords?
    *   What are some relevant long-tail keywords?
5.  **Specific RSS Feeds:** Can you list at least 5-10 trusted RSS feeds for this niche (blogs, official organizations, news sites)?
6.  **Relevant Reddit Subreddits:** Can you list 3-5 relevant Reddit subreddits?
7.  **General News Query (for NewsAPI):** Formulate a broad query string for NewsAPI to capture general news related to the niche (e.g., `'(triathlon OR ironman) AND (news OR race OR training)'`).
8.  **Negative Keywords:** Are there any specific keywords that should *exclude* an article (e.g., for a "Sustainable Technology" site, perhaps "cryptocurrency mining" if not desired)?
9.  **Excluded Sites/Domains:** Are there any specific websites that should be ignored?

**Configuration Steps:**

Once the above information is gathered:

1.  **Update `aggregator.py` Constants:**
    *   `NEWS_API_QUERY`: Set based on question #7.
    *   `SUBREDDITS`: Set based on question #6.
    *   `MAX_REDDIT_POSTS_PER_SUB`: Adjust as needed (default is low, can be increased).
    *   `RSS_FEEDS`: Populate the dictionary with niche-specific feeds from question #5.
        ```python
        RSS_FEEDS = {
            "Niche Source 1 Name": "URL_to_RSS_feed_1",
            "Niche Source 2 Name": "URL_to_RSS_feed_2",
            # ... add all feeds
        }
        ```
    *   `CATEGORIES`: Define a list of category names based on question #3.
        ```python
        CATEGORIES = [
            'Niche Category 1',
            'Niche Category 2',
            # ... add all categories
        ]
        ```
    *   `CATEGORY_KEYWORDS`: This is crucial. Create a dictionary where keys are the `CATEGORIES` defined above, and values are lists of keywords that assign an article to that category. This often requires iteration and refinement.
        ```python
        CATEGORY_KEYWORDS = {
            'Niche Category 1': ['keyword1', 'keyword2', 'phrase for category 1'],
            'Niche Category 2': ['keyword3', 'keyword4'],
            # ... for all categories
        }
        ```
    *   `TRENDING_KEYWORDS`: (Optional) Keywords that might push an article into a "Trending" category.
    *   `MAX_HEADLINE_WORDS`: Adjust if needed (default: 8).

2.  **Update `config.json`:**
    *   `excluded_sites`: Add any domains to ignore based on question #9.
    *   `negative_keywords`: Add keywords that should filter out articles, based on question #8.
    *   `low_quality_url_patterns`: Keep the defaults unless specific patterns for the niche are known.

3.  **Update `template.html` (and `archive_index_template.html`):**
    *   **Site Title:** Change `<title>AI Flash Report - Daily AI News</title>` to reflect the new niche and site name.
    *   **Meta Description & Keywords:**
        *   `<meta name="description" content="...">`: Update with a compelling description using primary and secondary keywords for the new niche.
        *   `<meta name="keywords" content="...">`: Update with the target keywords from question #4.
    *   **Open Graph & Twitter Card Tags:**
        *   Update `og:title`, `og:description`, `og:image`, `og:url`, `og:site_name`.
        *   Update `twitter:title`, `twitter:description`, `twitter:image`.
        *   *Ensure placeholder image paths (`/images/og-image.png`, etc.) are noted so the user knows to create these images.*
    *   **Schema.org Structured Data:**
        *   Update `headline`, `publisher.name`, `publisher.logo.url`, and `description` to match the new niche.
        *   *Note the need for a `logo.png` in `/images/`.*
    *   **Site Header/Logo:** Update `<h1>` and any logo elements.
    *   **Footer:** Update copyright and any links.
    *   **Category Descriptions (if used in template):** If the template includes static text per category, ensure this is updated or made dynamic.

4.  **Update `README.md`:**
    *   Reflect the new project's name, niche, and purpose.
    *   Update setup instructions if any specific changes were made.

5.  **Update `generate_sitemap.py` & `sitemap_template.xml`:**
    *   Ensure the base URL in `sitemap_template.xml` (`<loc>https://aiflashreport.com...</loc>`) is changed to the new site's domain. The `generate_sitemap.py` script will use this template.

6.  **Update `robots.txt`:**
    *   Ensure `Sitemap:` directive points to the correct `sitemap.xml` URL for the new domain.

### Phase 2: Technical Setup

1.  **Clone/Copy Base Project:** Start with a copy of the original project files.
2.  **Python Virtual Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    The `requirements.txt` should contain:
    ```
    requests
    feedparser
    praw
    Jinja2
    langdetect
    # Add other specific versions if necessary
    ```
4.  **API Keys & Environment Variables:**
    The script requires API keys for NewsAPI and Reddit. These must be set as environment variables.
    *   `NEWS_API_KEY`
    *   `REDDIT_CLIENT_ID`
    *   `REDDIT_CLIENT_SECRET`
    *   `REDDIT_USER_AGENT` (e.g., `"web:yournicheaggregator:v0.1 (by /u/yourusername)"`)
    *   `PERPLEXITY_API_KEY` (Optional, can be removed if not used for the new niche)

    These can be set in the shell or a `.env` file (ensure `.env` is in `.gitignore`).

### Phase 3: Customization & Branding

1.  **CSS Styling:** Modify the CSS in `template.html` (or a linked `style.css` file if separated) to match the desired look and feel for the new niche.
2.  **Images:** Create and place the necessary images:
    *   `/images/og-image.png` (1200x630px)
    *   `/images/twitter-card.png` (1200x600px)
    *   `/images/logo.png` (for Schema.org and site header)
    *   Favicon files (e.g., `favicon.ico`, `favicon-16x16.png`, etc.)
3.  **Content for Static Pages:**
    *   Update `about.html` with relevant information for the new site.
    *   Create any other static pages needed.

### Phase 3.5: Essential Forms & Boilerplate Pages

**Critical for Legal Compliance & User Engagement:**

1.  **Contact Form (`contact.html`):**
    *   **Purpose:** User feedback, content suggestions, partnership inquiries
    *   **Key Elements:**
        *   Contact form with fields: Name, Email, Subject (dropdown), Message
        *   Netlify form integration: `data-netlify="true"` and honeypot protection
        *   Form action pointing to success page: `action="/contact-success.html"`
        *   Professional styling matching site theme
        *   Header styling consistent with main template
    *   **Customization for New Niche:**
        *   Update subject dropdown options relevant to the niche
        *   Modify intro text and contact reasons
        *   Update social media links in contact info section
        *   Ensure footer navigation matches other pages

2.  **Privacy Policy (`privacy.html`):**
    *   **Purpose:** Legal compliance for data collection (GDPR/CCPA)
    *   **Required Sections:**
        *   Data collection practices (newsletter, contact form)
        *   Analytics usage (Plausible, ConvertBox)
        *   Cookie usage and management
        *   User rights and data retention
        *   Contact information for privacy requests
    *   **Customization for New Niche:**
        *   Update company/site name throughout
        *   Modify data collection descriptions based on actual forms used
        *   Adjust contact information and jurisdiction

3.  **Cookie Policy (`cookie-policy.html`):**
    *   **Purpose:** Transparency about cookie usage
    *   **Required Sections:**
        *   Essential cookies (site functionality)
        *   Analytics cookies (Plausible)
        *   Marketing cookies (ConvertBox)
        *   Instructions for disabling cookies
        *   Cookie consent management
    *   **Customization for New Niche:**
        *   Update site name and domain references
        *   Modify based on actual tracking tools used

4.  **Newsletter Signup Integration:**
    *   **Location:** Embedded in main template (`template.html`)
    *   **Key Features:**
        *   Email input with validation
        *   Netlify form processing
        *   Privacy policy link
        *   Spam protection (honeypot)
        *   Analytics tracking for conversions
    *   **Customization for New Niche:**
        *   Update value proposition text
        *   Modify subscriber count claims
        *   Adjust frequency options if needed

5.  **Footer Navigation Consistency:**
    *   **Required Links:** Home, Archive Index, About, Contact
    *   **Legal Links:** Privacy Policy, Cookie Policy (when created)
    *   **Implementation:** Must be consistent across all pages:
        *   `index.html` (main page)
        *   `about.html`
        *   `contact.html`
        *   `template.html` (for generated pages)
        *   Archive pages (via `archive_index_template.html`)

6.  **Success/Thank You Pages:**
    *   **Contact Success (`contact-success.html`):** Confirmation after form submission
    *   **Newsletter Thank You (`thank-you.html`):** Confirmation after newsletter signup
    *   **Customization:** Update messaging and next steps relevant to niche

**Implementation Checklist for New Niche:**
- [ ] Copy all form and policy page templates
- [ ] Update site name, domain, and contact information
- [ ] Customize form fields and options for niche relevance
- [ ] Ensure consistent header styling across all pages
- [ ] Update footer navigation on all pages
- [ ] Test form submissions and success page redirects
- [ ] Review legal compliance for target jurisdiction
- [ ] Link privacy policy from newsletter signup form

### Phase 4: Automation & Deployment

1.  **GitHub Repository:** Create a new GitHub repository for the project.
2.  **GitHub Actions Workflow (`.github/workflows/daily-update.yml`):**
    *   This file can largely be reused.
    *   **Crucial:** Set up GitHub Secrets in the new repository settings for all the API keys used in the `env` section of the workflow (`NEWS_API_KEY`, `REDDIT_CLIENT_ID`, etc.).
3.  **Hosting:**
    *   **Static Hosting Providers:** Netlify, GitHub Pages, Vercel, Cloudflare Pages are good options.
    *   Configure the chosen provider to build/deploy from the `main` branch of the GitHub repository. Usually, no complex build command is needed as HTML is committed directly.

### Phase 5: Testing, Maintenance & Iteration

1.  **Local Testing:** Run `python3 aggregator.py` locally to test data fetching, processing, and HTML generation. View `index.html` in a browser.
2.  **Deployment Testing:** Push changes to GitHub and monitor the GitHub Actions workflow. Check the live site after deployment.
3.  **Ongoing Maintenance:**
    *   Monitor for broken RSS feeds or API issues.
    *   Refine `CATEGORY_KEYWORDS` and `negative_keywords` based on the quality of aggregated content.
    *   Update dependencies and Python version as needed.
    *   Check Google Search Console for SEO issues and performance.

## 4. Developer & Project Rules (Best Practices)

These are inherited from the original project and are crucial for smooth development, especially with automated content generation.

*   **Git Workflow:**
    *   **Main Branch:** The `main` branch is the source of truth and what gets deployed.
    *   **Feature Branches:** Make changes on separate branches and merge via Pull Requests (optional for solo developers but good practice).
    *   **API Keys:** NEVER commit API keys or sensitive credentials. Use environment variables locally and GitHub Secrets for Actions.
    *   **Generated Files:**
        *   `index.html`, `archive/*.html`, `sitemap.xml` are auto-generated.
        *   **DO NOT MANUALLY EDIT THESE FILES.**
        *   Before committing changes to *code* (`aggregator.py`) or *templates* (`template.html`), if you've run the script locally, **you MUST restore/discard changes to these generated files**:
            ```bash
            git restore index.html archive/ sitemap.xml
            # or git checkout -- index.html archive/ sitemap.xml
            ```
        *   The GitHub Action is responsible for generating the final versions of these files.
    *   **Pull Before Push:** Always `git pull --rebase` (or `git pull`) before pushing changes to avoid conflicts, especially since the remote can be updated by the GitHub Action.
*   **Code Standards:**
    *   Keep `aggregator.py` well-commented, especially the data fetching and categorization logic.
    *   Ensure robust error handling (e.g., what happens if an RSS feed is down or an API key is invalid?). The script should ideally continue with other sources.
    *   Make configuration (keywords, sources) easy to update, primarily through `aggregator.py` constants and `config.json`.
*   **SEO Maintenance:**
    *   Regularly review meta descriptions and keywords in `template.html`.
    *   Submit `sitemap.xml` to Google Search Console.
    *   Monitor indexing status and crawl errors.

## 5. Example: Adapting for "Sustainable Technology News"

**AI/User Answers:**
1.  **Primary Niche:** Sustainable Technology
2.  **Site Name:** GreenTech Today
3.  **Key Categories:** Renewable Energy, Green Transportation, Sustainable Building, Circular Economy, Climate Tech
4.  **Keywords:**
    *   Primary: sustainable technology, green tech news, climate tech
    *   Secondary: renewable energy, electric vehicles, sustainable innovation
    *   Long-tail: latest advancements in solar power, circular economy business models
5.  **RSS Feeds:** `{"CleanTechnica": "...", "GreenBiz": "...", ...}`
6.  **Subreddits:** `["sustainability", "ClimateAction", "RenewableEnergy"]`
7.  **NewsAPI Query:** `'(sustainable OR sustainability OR "green tech" OR "climate tech" OR renewable) AND (technology OR innovation OR news OR breakthrough)'`
8.  **Negative Keywords:** `["politics", "fossil fuels investment"]`
9.  **Excluded Sites:** `["old-biased-site.com"]`

**AI/Developer Actions:**
*   Updates `RSS_FEEDS`, `CATEGORIES`, `CATEGORY_KEYWORDS`, `NEWS_API_QUERY` in `aggregator.py`.
*   Updates `negative_keywords`, `excluded_sites` in `config.json`.
*   Changes titles, meta tags, and branding in `template.html` to "GreenTech Today".
*   Creates `logo.png`, `og-image.png`, `twitter-card.png` with GreenTech Today branding.
*   Sets up new GitHub repo, secrets, and deploys via Netlify/GitHub Pages.

This guide provides a comprehensive framework. The key to success is careful configuration in Phase 1 and iterative refinement of keywords and sources based on the aggregated content quality.

## 6. Key Lessons Learned & Best Practices

This section documents critical insights from building and deploying automated news aggregation sites, providing guidance for future implementations across different niches.

### 6.1 Core Architecture Decisions

**Static Site Generation Approach:**
- **Advantage:** Static HTML files (`index.html`, daily archives `YYYY-MM-DD.html`, `archive.html`) provide excellent performance, SEO benefits, and simplified hosting
- **Tools:** Python + Jinja2 templating proved to be a lightweight yet powerful combination
- **Consideration:** Evaluate if static generation fits your niche requirements. For real-time updates or heavy user interaction, consider hybrid approaches

**Content Source Strategy:**
- **RSS Feeds:** Easier initial integration (no API keys), widely available, but quality varies
- **APIs (NewsAPI, Reddit):** More structured data and richer content, but require API key management, rate limits, and secure storage
- **Recommendation:** Start with RSS for simplicity, then add APIs as needed

**Content Quality Control:**
- **Essential Components:** Language filtering, duplicate detection (`processed_urls.json`), keyword-based categorization
- **Niche Adaptation:** Filtering and categorization logic must be tailored for each domain

### 6.2 Automation & Deployment

**GitHub Actions Best Practices:**
- **Workflow Benefits:** Excellent for daily content updates with automated checkout, Python setup, dependency installation, and file commits
- **Critical Pitfall:** Path handling in GitHub Action runners - commands execute from repository root
- **Required Permission:** Ensure `permissions: contents: write` for automated commits

**Static Content + SPA Integration:**
- **Key Directory:** Use `public/` directory in Vite projects - files are copied to build output root
- **Netlify Redirects:** Essential for SPA compatibility. Add specific redirect rules for static content paths before SPA catch-all rules:
  ```toml
  # Static content first
  /news/* /news/:splat 200
  /sitemap.xml /sitemap.xml 200
  # SPA catch-all last
  /* /index.html 200
  ```

**SEO Implementation:**
- **Sitemap Generation:** Separate Python script (`generate_sitemap.py`) for valid XML format
- **Common Error:** XML syntax strictness - avoid f-string backslash issues
- **Automation:** Integrate sitemap updates into GitHub Actions workflow

### 6.3 Development Workflow

**Git Management with Automation:**
- **Sync Issues:** GitHub Actions create commits that can desync local repositories
- **Best Practices:**
  - Always `git pull --rebase` before starting work and before pushing
  - Never manually edit auto-generated files (`index.html`, archives, `sitemap.xml`)
  - Use `git restore <generated_files>` before committing code changes
  - Maintain clear separation between "source code" and "generated artifacts"

**Configuration Management:**
- **Recommended Approach:** Use `config.json` for RSS feeds, keywords, and filters
- **Benefits:** Enables updates without code changes, easier niche adaptation

**Testing & Debugging:**
- **Local Preview:** Use `python -m http.server` from output directory
- **Path Awareness:** Local server paths may differ from live site structure
- **Essential Tools:** GitHub Action logs, Netlify deploy logs, browser developer tools

### 6.4 Common Issues & Solutions

**Frequent Challenges:**
- Jinja2 undefined variables
- Git merge conflicts from automated commits
- Path issues in scripts/workflows
- SPA routing conflicts with static content
- XML syntax errors in sitemap generation

**Critical Success Factors:**
1. **Early Planning:** Design Netlify redirects before SPA integration
2. **Strict Git Workflow:** Document and enforce separation of source vs. generated files
3. **Thorough Testing:** Validate all generation scripts before automation integration
4. **Configuration-Driven Design:** Maximize adaptability for new niches

### 6.5 Recommendations for New Niches

**Pre-Development Checklist:**
- [ ] Research available RSS feeds for quality and update frequency
- [ ] Plan API integration strategy and secret management
- [ ] Design keyword categorization system for the niche
- [ ] Establish Git workflow documentation
- [ ] Plan static content routing strategy

**Implementation Priority:**
1. Start with RSS feeds for rapid prototyping
2. Implement robust filtering and categorization
3. Set up automated deployment pipeline
4. Add API sources for enhanced content
5. Optimize SEO and performance

This framework has proven effective for creating scalable, maintainable news aggregation sites that can be efficiently adapted across different niches and domains.
